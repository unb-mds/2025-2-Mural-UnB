# ‚öôÔ∏è Pr√©-processamento de Dados

O **pr√©-processamento** √© uma etapa fundamental em Machine Learning, pois garante que os dados estejam em um formato adequado para os algoritmos.  
Aqui abordaremos:

- Normaliza√ß√£o  
- Padroniza√ß√£o  
- Feature Scaling (com exemplo pr√°tico em KNN)  
- Codifica√ß√£o de vari√°veis  

---

## üîπ Normaliza√ß√£o

### O que √©?

A **normaliza√ß√£o** refere-se ao processo de ajustar valores em escalas diferentes para uma **escala comum**.  
Ela √© particularmente √∫til quando a distribui√ß√£o dos dados √© desconhecida ou n√£o segue uma distribui√ß√£o gaussiana.

### Principais tipos de normaliza√ß√£o

- **Min-Max (m√≠nimo e m√°ximo)**  
  Redimensiona os valores para um intervalo espec√≠fico, geralmente `[0, 1]`.  
  Exemplo: o menor valor vira `0`, o maior vira `1`.

- **Logarithmic Scaling (normaliza√ß√£o logar√≠tmica)**  
  Aplica uma transforma√ß√£o logar√≠tmica para reduzir o impacto de valores muito altos.

- **Escala decimal**  
  Desloca o ponto decimal para tornar os valores mais manej√°veis sem alterar as diferen√ßas relativas.

- **Normaliza√ß√£o da m√©dia (centraliza√ß√£o da m√©dia)**  
  Subtrai a m√©dia de cada valor, centralizando os dados em torno de `0`.

### Quando utilizar?

- Quando os dados possuem **distribui√ß√£o desconhecida ou n√£o-gaussiana**  
- Em **algoritmos baseados em dist√¢ncia** (KNN, SVM, KMeans), para evitar que vari√°veis com escalas maiores dominem os c√°lculos.

### Exemplo com Scikit-Learn

```python
from sklearn.preprocessing import MinMaxScaler
import numpy as np

# Exemplo: valores de tamanho de casas
X = np.array([[500], [1000], [1500], [2000]])

scaler = MinMaxScaler()
X_scaled = scaler.fit_transform(X)

print("Original:", X.ravel())
print("Normalizado:", X_scaled.ravel())
```

## üîπ Padroniza√ß√£o

Enquanto a normaliza√ß√£o ajusta os dados para um intervalo espec√≠fico, a padroniza√ß√£o transforma os dados para que tenham:

- M√©dia = 0

- Desvio padr√£o = 1

Essa t√©cnica √© tamb√©m conhecida como z-score scaling.

F√≥rmula
<p align="center"> <img src="./assets/formula_padronizacao.png" width="300"> </p>

Onde:

- X √© o valor original

- Œº √© a m√©dia

- œÉ √© o desvio padr√£o

Exemplo com Scikit-Learn
```python
from sklearn.preprocessing import StandardScaler
import numpy as np

# Exemplo: valores de pre√ßos de casas
X = np.array([[100000], [200000], [400000], [800000]])

scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)

print("Original:", X.ravel())
print("Padronizado:", np.round(X_scaled.ravel(), 2))
```
X √© o valor original,
mu √© a m√©dia do recurso, e
sigma √© o desvio padr√£o do recurso.
Essa f√≥rmula redimensiona os dados de forma que sua distribui√ß√£o tenha uma m√©dia de 0 e um desvio padr√£o de 1.
```python
from sklearn.preprocessing import StandardScaler
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)
```

üîπ Feature Scaling (Escalonamento de Vari√°veis)

O escalonamento de vari√°veis √© crucial em algoritmos baseados em dist√¢ncias, como o KNN.
A imagem abaixo mostra como os limites de decis√£o mudam drasticamente dependendo se os dados foram escalados ou n√£o.

<p> <img src="./assets/feature_scaling.png" width="700"> </p>

> Retirado da documenta√ß√£o oficial do [Scikit-Learn.org](https://scikit-learn.org/stable/auto_examples/preprocessing/plot_scaling_importance.html#sphx-glr-auto-examples-preprocessing-plot-scaling-importance-py)

No lado esquerdo, sem escalonamento, a vari√°vel "__proline__" domina a decis√£o do modelo, pois seus valores variam aproximadamente entre 0 e 1.000, enquanto a vari√°vel "__hue__" varia apenas entre 1 e 10. Isso faz com que diferen√ßas em ‚Äú__hue__‚Äù sejam praticamente ignoradas.

No lado direito, ap√≥s aplicar o StandardScaler, ambas as vari√°veis s√£o transformadas para uma escala aproximada entre -3 e 3, permitindo que o modelo leve em considera√ß√£o de forma equilibrada as duas vari√°veis. Assim, a estrutura de vizinhan√ßa muda completamente e os limites de decis√£o se tornam mais representativos.

üîπ Codifica√ß√£o de Vari√°veis

Preferencialmente feito utilizando pandas

Muitos algoritmos n√£o trabalham diretamente com vari√°veis categ√≥ricas, sendo necess√°rio convert√™-las em valores num√©ricos.

One-Hot Encoding

Cria colunas bin√°rias para cada categoria.
Exemplo: cor = [vermelho, azul, verde] vira cor_vermelho, cor_azul, cor_verde.
```python
import pandas as pd
from sklearn.preprocessing import OneHotEncoder

df = pd.DataFrame({"Cor": ["vermelho", "azul", "verde", "azul"]})

encoder = OneHotEncoder(sparse_output=False)
encoded = encoder.fit_transform(df[["Cor"]])

print(pd.DataFrame(encoded, columns=encoder.get_feature_names_out(["Cor"])))
```
Label Encoding

Atribui inteiros para cada categoria.
Exemplo: vermelho=0, azul=1, verde=2.
```python
from sklearn.preprocessing import LabelEncoder

le = LabelEncoder()
labels = le.fit_transform(df["Cor"])
print(labels)
```