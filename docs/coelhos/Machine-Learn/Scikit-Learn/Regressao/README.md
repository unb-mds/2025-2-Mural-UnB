# üìà Modelos de Regress√£o no Scikit-Learn

A **regress√£o** √© uma das tarefas fundamentais em Machine Learning, usada para prever valores num√©ricos cont√≠nuos com base em vari√°veis independentes.  

O Scikit-Learn oferece desde modelos lineares simples at√© t√©cnicas avan√ßadas de regulariza√ß√£o e ensemble.  

Neste guia, abordaremos:

- Regress√£o Linear  
- Regress√£o N√£o Linear
- Regress√£o Ridge  
- Regress√£o Lasso  
- Elastic Net  
- √Årvores de Regress√£o e Random Forest  
- Gradient Boosting Regressor  

---

## üîπ Regress√£o Linear

### O que √©?
Assume que existe uma rela√ß√£o linear entre as vari√°veis independentes (**X**) e a vari√°vel dependente (**y**).  

### F√≥rmula
\[
y = \beta_0 + \beta_1 x_1 + \beta_2 x_2 + \dots + \beta_n x_n
\]

Onde:  
- \( \beta_0 \) √© o intercepto  
- \( \beta_i \) s√£o os coeficientes  
- \( x_i \) s√£o as vari√°veis independentes  

### Exemplo em Python
```python
from sklearn.linear_model import LinearRegression
import numpy as np

# Dados de exemplo: horas de estudo (X) e nota final (y)
X = np.array([[1], [2], [3], [4], [5]])
y = np.array([50, 55, 65, 70, 80])

model = LinearRegression()
model.fit(X, y)

print("Coeficiente:", model.coef_)
print("Intercepto:", model.intercept_)
print("Previs√£o para 6 horas de estudo:", model.predict([[6]]))
```
<p align="center"> <img src="./assets/graph_linear_fit.png" width="400"> </p>

## üîπ Regress√£o N√£o Linear
**Nem toda rela√ß√£o √© linear**. Para casos n√£o lineares, podemos:

Transformar features: **aplicar polin√¥mios ou fun√ß√µes n√£o lineares**.

```python
from sklearn.preprocessing import PolynomialFeatures
from sklearn.linear_model import LinearRegression
from sklearn.pipeline import Pipeline

# Transformacao polinomial de grau 2
poly_model = Pipeline([
    ('poly', PolynomialFeatures(degree=2)),
    ('linear', LinearRegression())
])

poly_model.fit(X, y)
print("Previs√£o para 6 horas:", poly_model.predict([[6]]))
```

<p align="center"> <img src="./assets/graph_quadratic_fit.png" width="400"> </p>

## üîπ Regress√£o Ridge (L2)
### O que √©?
Aplica penaliza√ß√£o L2 aos coeficientes, evitando que fiquem muito grandes. Reduz overfitting.

```python
from sklearn.linear_model import Ridge

ridge = Ridge(alpha=1.0)
ridge.fit(X, y)

print("Coeficientes Ridge:", ridge.coef_)
```
## üîπ Regress√£o Lasso (L1)
### O que √©?
Aplica penaliza√ß√£o L1, podendo reduzir coeficientes irrelevantes a zero, realizando sele√ß√£o autom√°tica de vari√°veis.

```python
from sklearn.linear_model import Lasso

lasso = Lasso(alpha=0.1)
lasso.fit(X, y)

print("Coeficientes Lasso:", lasso.coef_)
```
## üîπ Elastic Net (L1 + L2)
### O que √©?
**Combina** as penaliza√ß√µes L1 (sele√ß√£o de vari√°veis) e L2 (estabilidade dos coeficientes).

```python
from sklearn.linear_model import ElasticNet

elastic = ElasticNet(alpha=0.1, l1_ratio=0.5)
elastic.fit(X, y)

print("Coeficientes ElasticNet:", elastic.coef_)
```

üìä Compara√ß√£o entre Modelos de Regress√£o
| Modelo            | Vantagens                                | Desvantagens                         | Quando Usar                            |
| ----------------- | ---------------------------------------- | ------------------------------------ | -------------------------------------- |
| Linear Regression | Simples, interpret√°vel                   | Limita√ß√µes em dados n√£o-lineares     | Rela√ß√µes lineares simples              |
| Ridge             | Reduz overfitting, coeficientes est√°veis | N√£o zera vari√°veis irrelevantes      | Muitos atributos correlacionados       |
| Lasso             | Sele√ß√£o de vari√°veis autom√°tica          | Pode eliminar vari√°veis relevantes   | Muitas vari√°veis irrelevantes          |
| Multi Variable Regression       | Consegue captar rela√ß√µes mais complexas          | Mais dif√≠cil ajustar e suset√≠vel a overfitting | Rela√ß√µes n√£o lineares     |

<sub>Documenta√ß√£o criada por **Tiago Bittencourt** ([@TiagoSBittencourt](https://github.com/TiagoSBittencourt))</sub>